{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Richness Index (LRI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "\n",
    "# Lexical Richness module\n",
    "# Documentation: https://pypi.org/project/lexicalrichness/\n",
    "from lexicalrichness import LexicalRichness\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## John Locke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2095552"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"LockeComplete.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    Locke = file.read()\n",
    "\n",
    "len(Locke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Richness of John Locke\n",
      "Unique Word Count: 11102\n",
      "Type Token Ratio: 0.028750411108607772\n",
      "Root Type Token Ratio: 17.865807122203112\n",
      "Corrected Type Token Ratio: 12.63303336748074\n",
      "Mean Segmental Type Token Ratio: 0.8731943545253649\n",
      "Moving Average Type Token Ratio: 0.8739445830012871\n",
      "Measure of Textual Lexical Diversity: 66.38223299845599\n",
      "Hypergeometric Distribution Diversity: 0.8476594498588272\n"
     ]
    }
   ],
   "source": [
    "# LRI of full text\n",
    "locke = LexicalRichness(Locke)\n",
    "\n",
    "print(\"Lexical Richness of John Locke\")\n",
    "\n",
    "# Return (unique) word count\n",
    "print(\"Unique Word Count: %s\" % locke.terms)\n",
    "\n",
    "# Return Type Token Ratio (TTR) of text\n",
    "print(\"Type Token Ratio: %s\" % locke.ttr)\n",
    "\n",
    "# Return Root Type Token Ratio (RTTR) of text\n",
    "print(\"Root Type Token Ratio: %s\" % locke.rttr)\n",
    "\n",
    "# Return Corrected Type Token Ratio (CTTR) of text\n",
    "print(\"Corrected Type Token Ratio: %s\" % locke.cttr)\n",
    "\n",
    "# Return Mean Segmental Type Token Ratio (MSTTR) of text\n",
    "print(\"Mean Segmental Type Token Ratio: %s\" % locke.msttr(segment_window=25))\n",
    "\n",
    "# Return Moving Average Type Token Ratio (MATTR) of text\n",
    "print(\"Moving Average Type Token Ratio: %s\" % locke.mattr(window_size=25))\n",
    "\n",
    "# Return Measure of Textual Lexical Diversity (MTLD)\n",
    "print(\"Measure of Textual Lexical Diversity: %s\" % locke.mtld(threshold=0.72))\n",
    "\n",
    "# Return hypergeometric distribution diversity (HD-D) measure.\n",
    "print(\"Hypergeometric Distribution Diversity: %s\" % locke.hdd(draws=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRI Mean Average Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Word Count: 1886.8\n",
      "Type Token Ratio: 0.19\n",
      "Root Type Token Ratio: 18.87\n",
      "Corrected Type Token Ratio: 13.34\n",
      "Mean Segmental Type Token Ratio: 0.9\n",
      "Moving Average Type Token Ratio: 0.9\n",
      "Measure of Textual Lexical Diversity: 106.85\n",
      "Hypergeometric Distribution Diversity: 0.85\n"
     ]
    }
   ],
   "source": [
    "# For accurate comparison, Jockers recommends comparing random 10,000 word chunks of each corpus\n",
    "# Use without stopwords as all vocabulary matters here\n",
    "# Build a function to select 10,000 random words and find mean average of multiple LRIs\n",
    "def LRI (times, text):\n",
    "    \n",
    "    # Empty variables for LRI mean averages\n",
    "    UWQavg = []\n",
    "    TTRavg = []\n",
    "    RTTRavg = []\n",
    "    CTTRavg = []\n",
    "    MSTTRavg = []\n",
    "    MATTRavg = []\n",
    "    MTLDavg = []\n",
    "    HDDavg = []\n",
    "    \n",
    "    # Tokenize text for randomization with NLTK\n",
    "    textToke = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Iterate through the function multiple times\n",
    "    for i in range(times):\n",
    "        \n",
    "        # Pick 10000 random words via RANDOM\n",
    "        textRand = random.sample(textToke, 10000)\n",
    "    \n",
    "        # Convert back to string\n",
    "        textStr = ' '.join(textRand)\n",
    "    \n",
    "        # Perform LRI with LexicalRichness\n",
    "        textLRI = LexicalRichness(textStr)\n",
    "    \n",
    "        # Unique Word Count\n",
    "        UWQ = textLRI.terms\n",
    "        # Type Token Ratio \n",
    "        TTR = textLRI.ttr\n",
    "        # Root Type Token Ratio \n",
    "        RTTR = textLRI.rttr\n",
    "        # Corrected Type Token Ratio \n",
    "        CTTR = textLRI.cttr\n",
    "        # Mean Segmental Type Token Ratio \n",
    "        MSTTR = textLRI.msttr(segment_window=25)\n",
    "        # Return Moving Average Type Token Ratio (MATTR) of text\n",
    "        MATTR = textLRI.mattr(window_size=25)\n",
    "        # Measure of Textual Lexical Diversity \n",
    "        MTLD = textLRI.mtld(threshold=0.72)\n",
    "        # Hypergeometric Distribution Diversity measure\n",
    "        HDD = textLRI.hdd(draws=42)\n",
    "        \n",
    "        # Append results for mean average\n",
    "        UWQavg.append(UWQ)\n",
    "        TTRavg.append(TTR)\n",
    "        RTTRavg.append(RTTR)\n",
    "        CTTRavg.append(CTTR)\n",
    "        MSTTRavg.append(MSTTR)\n",
    "        MATTRavg.append(MATTR)\n",
    "        MTLDavg.append(MTLD)\n",
    "        HDDavg.append(HDD)\n",
    "                                         # Average results, round to 2 decimal places\n",
    "    print(\"Unique Word Count: %s\" % round(sum(UWQavg)/len(UWQavg), 2))\n",
    "    print(\"Type Token Ratio: %s\" % round(sum(TTRavg)/len(TTRavg), 2))\n",
    "    print(\"Root Type Token Ratio: %s\" % round(sum(RTTRavg)/len(RTTRavg), 2))\n",
    "    print(\"Corrected Type Token Ratio: %s\" % round(sum(CTTRavg)/len(CTTRavg), 2))\n",
    "    print(\"Mean Segmental Type Token Ratio: %s\" % round(sum(MSTTRavg)/len(MSTTRavg), 2))\n",
    "    print(\"Moving Average Type Token Ratio: %s\" % round(sum(MATTRavg)/len(MATTRavg), 2))\n",
    "    print(\"Measure of Textual Lexical Diversity: %s\" % round(sum(MTLDavg)/len(MTLDavg), 2))\n",
    "    print(\"Hypergeometric Distribution Diversity: %s\" % round(sum(HDDavg)/len(HDDavg), 2))\n",
    "    return;\n",
    "\n",
    "# Iterate through function multiple times and average results\n",
    "LRI(10, Locke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency and Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Write out new file with stopwords removed for each text file.\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Build stopword files\n",
    "\n",
    "file = open(\"Locke_HumanUnderstandingCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Locke_HumanUnderstandingSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Locke_TwoTreatisesCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Locke_TwoTreatisesSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"LockeComplete.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('LockeCompleteSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open .txt files with stopwords removed\n",
    "with open(\"Locke_HumanUnderstandingSTOPWORDS.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    LockeHum = file.read()\n",
    "    \n",
    "with open(\"Locke_TwoTreatisesSTOPWORDS.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    LockeTwo = file.read()\n",
    "    \n",
    "with open(\"LockeCompleteSTOPWORDS.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    LockeCom = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ideas', 21319), ('one', 13407), ('idea', 11485), ('mind', 7894), ('may', 7831), ('knowledge', 7064), ('man', 7027), ('us', 6812), ('men', 6740), ('things', 6683)]\n",
      "[('power', 5411), ('right', 3983), ('one', 3738), ('government', 2373), ('men', 2310), ('may', 2268), ('man', 2233), ('god', 2002), ('adam', 1953), ('nature', 1939)]\n",
      "[('ideas', 18438), ('one', 15323), ('idea', 9982), ('may', 9009), ('man', 8274), ('power', 8267), ('men', 8029), ('us', 7091), ('mind', 6846), ('things', 6286)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, Word Frequency and Count function\n",
    "def WordFreq (text):\n",
    "    textToke = nltk.word_tokenize(text)\n",
    "    textWord = nltk.Text(textToke)\n",
    "    textFreq = nltk.FreqDist(textWord)\n",
    "    textCount = Counter(textFreq)\n",
    "    print(Counter(textCount).most_common(10))\n",
    "    return;\n",
    "\n",
    "WordFreq(LockeHum)\n",
    "WordFreq(LockeTwo)\n",
    "WordFreq(LockeCom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## George Berkeley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1021784"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"BerkeleyComplete.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    Berkeley = file.read()\n",
    "\n",
    "len(Berkeley)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Richness of George Berkeley\n",
      "Unique Word Count: 9531\n",
      "Type Token Ratio: 0.051520589856968335\n",
      "Root Type Token Ratio: 22.1594842432482\n",
      "Corrected Type Token Ratio: 15.66912157599725\n",
      "Mean Segmental Type Token Ratio: 0.8864549263414692\n",
      "Moving Average Type Token Ratio: 0.885777369303587\n",
      "Measure of Textual Lexical Diversity: 80.28268012005267\n",
      "Hypergeometric Distribution Diversity: 0.8592101099541077\n"
     ]
    }
   ],
   "source": [
    "berkeley = LexicalRichness(Berkeley)\n",
    "\n",
    "# Return word count\n",
    "print(\"Lexical Richness of George Berkeley\")\n",
    "\n",
    "# Return (unique) word count\n",
    "print(\"Unique Word Count: %s\" % berkeley.terms)\n",
    "\n",
    "# Return Type Token Ratio (TTR) of text\n",
    "print(\"Type Token Ratio: %s\" % berkeley.ttr)\n",
    "\n",
    "# Return Root Type Token Ratio (RTTR) of text\n",
    "print(\"Root Type Token Ratio: %s\" % berkeley.rttr)\n",
    "\n",
    "# Return Corrected Type Token Ratio (CTTR) of text\n",
    "print(\"Corrected Type Token Ratio: %s\" % berkeley.cttr)\n",
    "\n",
    "# Return Mean Segmental Type Token Ratio (MSTTR) of text\n",
    "print(\"Mean Segmental Type Token Ratio: %s\" % berkeley.msttr(segment_window=25))\n",
    "\n",
    "# Return Moving Average Type Token Ratio (MATTR) of text\n",
    "print(\"Moving Average Type Token Ratio: %s\" % berkeley.mattr(window_size=25))\n",
    "\n",
    "# Return Measure of Textual Lexical Diversity (MTLD)\n",
    "print(\"Measure of Textual Lexical Diversity: %s\" % berkeley.mtld(threshold=0.72))\n",
    "\n",
    "# Return hypergeometric distribution diversity (HD-D) measure.\n",
    "print(\"Hypergeometric Distribution Diversity: %s\" % berkeley.hdd(draws=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Word Count: 2133.7\n",
      "Type Token Ratio: 0.21\n",
      "Root Type Token Ratio: 21.34\n",
      "Corrected Type Token Ratio: 15.09\n",
      "Mean Segmental Type Token Ratio: 0.91\n",
      "Moving Average Type Token Ratio: 0.91\n",
      "Measure of Textual Lexical Diversity: 125.97\n",
      "Hypergeometric Distribution Diversity: 0.86\n"
     ]
    }
   ],
   "source": [
    "# LRI Mean Average Function\n",
    "LRI(10, Berkeley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency and Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file = open(\"Berkeley_AlciphronCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Berkeley_AlciphronSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Berkeley_HumanKnowledgeCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Berkeley_HumanKnowledgeSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Berkeley_TheoryOfVisionCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Berkeley_TheoryOfVisionSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Berkeley_ThreeDialoguesCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Berkeley_ThreeDialoguesSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"BerkeleyComplete.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('BerkeleyCompleteSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open .txt files with stopwords removed\n",
    "with open(\"Berkeley_AlciphronSTOPWORDS.txt\", \"r\") as file:\n",
    "    BerkeleyAlc = file.read()\n",
    "    \n",
    "with open(\"Berkeley_HumanKnowledgeSTOPWORDS.txt\", \"r\") as file:\n",
    "    BerkeleyHum = file.read()\n",
    "    \n",
    "with open(\"Berkeley_TheoryOfVisionSTOPWORDS.txt\", \"r\") as file:\n",
    "    BerkeleyThe = file.read()\n",
    "    \n",
    "with open(\"Berkeley_ThreeDialoguesSTOPWORDS.txt\", \"r\") as file:\n",
    "    BerkeleyThr = file.read()\n",
    "    \n",
    "with open(\"BerkeleyCompleteSTOPWORDS.txt\", \"r\") as file:\n",
    "    BerkeleyCom = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('men', 1895), ('man', 1480), ('things', 1300), ('one', 1150), ('god', 1110), ('religion', 1035), ('would', 1020), ('good', 975), ('dont', 955), ('cant', 955)]\n",
      "[('ideas', 2620), ('mind', 1880), ('may', 1870), ('things', 1530), ('idea', 1480), ('without', 1250), ('sense', 1170), ('one', 1100), ('motion', 1020), ('thing', 1000)]\n",
      "[('distance', 1990), ('visible', 1740), ('sight', 1590), ('object', 1500), ('objects', 1380), ('ideas', 1300), ('tangible', 1300), ('one', 1220), ('eye', 1150), ('magnitude', 1020)]\n",
      "[('things', 2620), ('mind', 2170), ('ideas', 1790), ('dont', 1680), ('matter', 1500), ('exist', 1410), ('existence', 1360), ('perceived', 1330), ('sensible', 1320), ('think', 1310)]\n",
      "[('things', 4004), ('ideas', 3847), ('mind', 3543), ('one', 2877), ('may', 2658), ('men', 2294), ('idea', 2268), ('sense', 2240), ('think', 2144), ('see', 2123)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, Word Frequency and Count \n",
    "WordFreq(BerkeleyAlc)\n",
    "WordFreq(BerkeleyHum)\n",
    "WordFreq(BerkeleyThe)\n",
    "WordFreq(BerkeleyThr)\n",
    "WordFreq(BerkeleyCom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## David Hume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2151680"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"HumeComplete.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    Hume = file.read()\n",
    "\n",
    "len(Hume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Richness of David Hume\n",
      "Unique Word Count: 18158\n",
      "Type Token Ratio: 0.04855599529361429\n",
      "Root Type Token Ratio: 29.693092842300015\n",
      "Corrected Type Token Ratio: 20.996187303192077\n",
      "Mean Segmental Type Token Ratio: 0.8842465570262564\n",
      "Moving Average Type Token Ratio: 0.8841193145365478\n",
      "Measure of Textual Lexical Diversity: 86.84633597280066\n",
      "Hypergeometric Distribution Diversity: 0.8555445898281581\n"
     ]
    }
   ],
   "source": [
    "hume = LexicalRichness(Hume)\n",
    "\n",
    "# Return word count\n",
    "print(\"Lexical Richness of David Hume\")\n",
    "\n",
    "# Return (unique) word count\n",
    "print(\"Unique Word Count: %s\" % hume.terms)\n",
    "\n",
    "# Return Type Token Ratio (TTR) of text\n",
    "print(\"Type Token Ratio: %s\" % hume.ttr)\n",
    "\n",
    "# Return Root Type Token Ratio (RTTR) of text\n",
    "print(\"Root Type Token Ratio: %s\" % hume.rttr)\n",
    "\n",
    "# Return Corrected Type Token Ratio (CTTR) of text\n",
    "print(\"Corrected Type Token Ratio: %s\" % hume.cttr)\n",
    "\n",
    "# Return Mean Segmental Type Token Ratio (MSTTR) of text\n",
    "print(\"Mean Segmental Type Token Ratio: %s\" % hume.msttr(segment_window=25))\n",
    "\n",
    "# Return Moving Average Type Token Ratio (MATTR) of text\n",
    "print(\"Moving Average Type Token Ratio: %s\" % hume.mattr(window_size=25))\n",
    "\n",
    "# Return Measure of Textual Lexical Diversity (MTLD)\n",
    "print(\"Measure of Textual Lexical Diversity: %s\" % hume.mtld(threshold=0.72))\n",
    "\n",
    "# Return hypergeometric distribution diversity (HD-D) measure.\n",
    "print(\"Hypergeometric Distribution Diversity: %s\" % hume.hdd(draws=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Word Count: 2706.5\n",
      "Type Token Ratio: 0.27\n",
      "Root Type Token Ratio: 27.07\n",
      "Corrected Type Token Ratio: 19.14\n",
      "Mean Segmental Type Token Ratio: 0.9\n",
      "Moving Average Type Token Ratio: 0.9\n",
      "Measure of Textual Lexical Diversity: 135.67\n",
      "Hypergeometric Distribution Diversity: 0.86\n"
     ]
    }
   ],
   "source": [
    "# LRI Mean Average Function\n",
    "LRI(10, Hume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency and Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file = open(\"Hume_EssaysMoralPoliticalLiteraryCLEAN.txt\", encoding=\"utf-8\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Hume_EssaysMoralPoliticalLiterarySTOPWORDS.txt','a', encoding=\"utf-8\") \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Hume_HumanUnderstandingCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Hume_HumanUnderstandingSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Hume_NaturalReligionCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Hume_NaturalReligionSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Hume_SourcesofMoralsCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Hume_SourcesofMoralsSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"HumeComplete.txt\", encoding=\"utf-8\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('HumeCompleteSTOPWORDS.txt','a', encoding=\"utf-8\") \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open .txt files with stopwords removed\n",
    "with open(\"Hume_EssaysMoralPoliticalLiterarySTOPWORDS.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    HumeEss = file.read()\n",
    "    \n",
    "with open(\"Hume_HumanUnderstandingSTOPWORDS.txt\", \"r\") as file:\n",
    "    HumeHum = file.read()\n",
    "    \n",
    "with open(\"Hume_NaturalReligionSTOPWORDS.txt\", \"r\") as file:\n",
    "    HumeNat = file.read()\n",
    "    \n",
    "with open(\"Hume_SourcesofMoralsSTOPWORDS.txt\", \"r\") as file:\n",
    "    HumeSou = file.read()\n",
    "    \n",
    "with open(\"HumeCompleteSTOPWORDS.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    HumeCom = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('may', 3292), ('one', 2596), ('every', 2304), ('great', 2252), ('must', 2112), ('would', 2036), ('men', 1808), ('much', 1764), ('government', 1748), ('people', 1648)]\n",
      "[('may', 1096), ('one', 760), ('nature', 736), ('us', 644), ('must', 624), ('experience', 588), ('mind', 564), ('cause', 552), ('human', 512), ('effect', 472)]\n",
      "[('human', 552), ('one', 512), ('world', 460), ('nature', 456), ('would', 424), ('god', 412), ('reason', 400), ('cause', 396), ('us', 392), ('cleanthes', 392)]\n",
      "[('us', 660), ('would', 608), ('one', 604), ('society', 524), ('man', 524), ('human', 480), ('justice', 464), ('sentiment', 452), ('even', 428), ('general', 416)]\n",
      "[('may', 3723), ('one', 3354), ('every', 2625), ('would', 2580), ('must', 2514), ('us', 2232), ('great', 2115), ('nature', 2073), ('even', 1989), ('men', 1935)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, Word Frequency and Count \n",
    "WordFreq(HumeEss)\n",
    "WordFreq(HumeHum)\n",
    "WordFreq(HumeNat)\n",
    "WordFreq(HumeSou)\n",
    "WordFreq(HumeCom)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
