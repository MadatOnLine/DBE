{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Richness Index (LRI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "\n",
    "# Lexical Richness module\n",
    "# Documentation: https://pypi.org/project/lexicalrichness/\n",
    "from lexicalrichness import LexicalRichness\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## John Locke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2040978"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"FullText/LockeComplete.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    Locke = file.read()\n",
    "\n",
    "len(Locke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Richness of John Locke\n",
      "Unique Word Count: 8800\n",
      "Type Token Ratio: 0.022749305241388226\n",
      "Root Type Token Ratio: 14.148988872856478\n",
      "Corrected Type Token Ratio: 10.00484597892982\n",
      "Mean Segmental Type Token Ratio: 0.8591416752843168\n",
      "Moving Average Type Token Ratio: 0.8593303533353228\n",
      "Measure of Textual Lexical Diversity: 56.95987689180785\n",
      "Hypergeometric Distribution Diversity: 0.8255656836997587\n"
     ]
    }
   ],
   "source": [
    "# LRI of full text\n",
    "locke = LexicalRichness(Locke)\n",
    "\n",
    "print(\"Lexical Richness of John Locke\")\n",
    "\n",
    "# Return (unique) word count\n",
    "print(\"Unique Word Count: %s\" % locke.terms)\n",
    "\n",
    "# Return Type Token Ratio (TTR) of text\n",
    "print(\"Type Token Ratio: %s\" % locke.ttr)\n",
    "\n",
    "# Return Root Type Token Ratio (RTTR) of text\n",
    "print(\"Root Type Token Ratio: %s\" % locke.rttr)\n",
    "\n",
    "# Return Corrected Type Token Ratio (CTTR) of text\n",
    "print(\"Corrected Type Token Ratio: %s\" % locke.cttr)\n",
    "\n",
    "# Return Mean Segmental Type Token Ratio (MSTTR) of text\n",
    "print(\"Mean Segmental Type Token Ratio: %s\" % locke.msttr(segment_window=25))\n",
    "\n",
    "# Return Moving Average Type Token Ratio (MATTR) of text\n",
    "print(\"Moving Average Type Token Ratio: %s\" % locke.mattr(window_size=25))\n",
    "\n",
    "# Return Measure of Textual Lexical Diversity (MTLD)\n",
    "print(\"Measure of Textual Lexical Diversity: %s\" % locke.mtld(threshold=0.72))\n",
    "\n",
    "# Return hypergeometric distribution diversity (HD-D) measure.\n",
    "print(\"Hypergeometric Distribution Diversity: %s\" % locke.hdd(draws=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRI Mean Average Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Word Count: 1615.1\n",
      "Type Token Ratio: 0.16\n",
      "Root Type Token Ratio: 16.15\n",
      "Corrected Type Token Ratio: 11.42\n",
      "Mean Segmental Type Token Ratio: 0.88\n",
      "Moving Average Type Token Ratio: 0.88\n",
      "Measure of Textual Lexical Diversity: 85.28\n",
      "Hypergeometric Distribution Diversity: 0.83\n"
     ]
    }
   ],
   "source": [
    "# For accurate comparison, Jockers recommends comparing random 10,000 word chunks of each corpus\n",
    "# Use without stopwords as all vocabulary matters here\n",
    "# Build a function to select 10,000 random words and find mean average of multiple LRIs\n",
    "def LRI (times, text):\n",
    "    \n",
    "    # Empty variables for LRI mean averages\n",
    "    UWQavg = []\n",
    "    TTRavg = []\n",
    "    RTTRavg = []\n",
    "    CTTRavg = []\n",
    "    MSTTRavg = []\n",
    "    MATTRavg = []\n",
    "    MTLDavg = []\n",
    "    HDDavg = []\n",
    "    \n",
    "    # Tokenize text for randomization with NLTK\n",
    "    textToke = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Iterate through the function multiple times\n",
    "    for i in range(times):\n",
    "        \n",
    "        # Pick 10000 random words via RANDOM\n",
    "        textRand = random.sample(textToke, 10000)\n",
    "    \n",
    "        # Convert back to string\n",
    "        textStr = ' '.join(textRand)\n",
    "    \n",
    "        # Perform LRI with LexicalRichness\n",
    "        textLRI = LexicalRichness(textStr)\n",
    "    \n",
    "        # Unique Word Count\n",
    "        UWQ = textLRI.terms\n",
    "        # Type Token Ratio \n",
    "        TTR = textLRI.ttr\n",
    "        # Root Type Token Ratio \n",
    "        RTTR = textLRI.rttr\n",
    "        # Corrected Type Token Ratio \n",
    "        CTTR = textLRI.cttr\n",
    "        # Mean Segmental Type Token Ratio \n",
    "        MSTTR = textLRI.msttr(segment_window=25)\n",
    "        # Return Moving Average Type Token Ratio (MATTR) of text\n",
    "        MATTR = textLRI.mattr(window_size=25)\n",
    "        # Measure of Textual Lexical Diversity \n",
    "        MTLD = textLRI.mtld(threshold=0.72)\n",
    "        # Hypergeometric Distribution Diversity measure\n",
    "        HDD = textLRI.hdd(draws=42)\n",
    "        \n",
    "        # Append results for mean average\n",
    "        UWQavg.append(UWQ)\n",
    "        TTRavg.append(TTR)\n",
    "        RTTRavg.append(RTTR)\n",
    "        CTTRavg.append(CTTR)\n",
    "        MSTTRavg.append(MSTTR)\n",
    "        MATTRavg.append(MATTR)\n",
    "        MTLDavg.append(MTLD)\n",
    "        HDDavg.append(HDD)\n",
    "                                         # Average results, round to 2 decimal places\n",
    "    print(\"Unique Word Count: %s\" % round(sum(UWQavg)/len(UWQavg), 2))\n",
    "    print(\"Type Token Ratio: %s\" % round(sum(TTRavg)/len(TTRavg), 2))\n",
    "    print(\"Root Type Token Ratio: %s\" % round(sum(RTTRavg)/len(RTTRavg), 2))\n",
    "    print(\"Corrected Type Token Ratio: %s\" % round(sum(CTTRavg)/len(CTTRavg), 2))\n",
    "    print(\"Mean Segmental Type Token Ratio: %s\" % round(sum(MSTTRavg)/len(MSTTRavg), 2))\n",
    "    print(\"Moving Average Type Token Ratio: %s\" % round(sum(MATTRavg)/len(MATTRavg), 2))\n",
    "    print(\"Measure of Textual Lexical Diversity: %s\" % round(sum(MTLDavg)/len(MTLDavg), 2))\n",
    "    print(\"Hypergeometric Distribution Diversity: %s\" % round(sum(HDDavg)/len(HDDavg), 2))\n",
    "    return;\n",
    "\n",
    "# Iterate through function multiple times and average results\n",
    "LRI(10, Locke)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency and Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'because', 'more', 'our', 'but', \"didn't\", 'which', 'does', \"mustn't\", 'will', 'needn', 'very', 'are', 'on', 'd', 'its', 'hers', 'ourselves', 'in', 'have', 'after', 'it', 'his', 'an', 'a', 'through', 'than', 're', 'won', 'doing', 'haven', 'himself', \"wouldn't\", 'whom', 'couldn', \"won't\", \"should've\", \"wasn't\", 'having', \"hasn't\", 'out', 'can', \"weren't\", 'he', 'doesn', 't', 'ma', 'yourself', \"aren't\", \"mightn't\", 'against', 'once', 'why', 'until', 'y', 'and', 'has', 'had', \"that'll\", 'some', 'been', \"it's\", 'should', 'didn', 'him', 'shouldn', 'mightn', 'as', 've', 'themselves', \"couldn't\", 'this', 'for', 'while', 'wasn', \"don't\", 'here', 'is', 'how', 'these', 'who', 'do', 'her', 'off', 'to', \"haven't\", 'or', 'be', 's', 'other', 'was', 'their', \"shouldn't\", 'into', 'o', 'not', 'just', 'what', 'most', 'too', 'during', 'all', 'i', 'them', 'about', 'hadn', 'shan', 'wouldn', 'your', 'up', 'we', 'that', 'above', 'those', \"doesn't\", 'where', 'both', 'then', 'again', 'down', 'herself', \"you're\", 'below', 'yours', 'so', 'at', 'itself', 'over', 'ours', 'ain', 'of', 'before', 'there', 'hasn', 'she', 'being', 'were', 'when', \"isn't\", 'theirs', 'the', 'am', 'further', 'each', 'm', 'mustn', 'such', 'myself', 'if', 'by', 'between', 'my', 'now', \"you've\", 'from', 'aren', 'any', \"hadn't\", 'you', \"shan't\", 'they', 'under', 'own', 'yourselves', 'same', \"you'll\", 'me', 'nor', \"she's\", 'few', 'isn', 'with', \"needn't\", 'did', 'don', \"you'd\", 'weren', 'no', 'll', 'only'}\n"
     ]
    }
   ],
   "source": [
    "#Write out new file with stopwords removed for each text file.\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build stopword files\n",
    "\n",
    "file = open(\"Clean/Locke_HumanUnderstandingCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Stopword/Locke_HumanUnderstandingSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Clean/Locke_TwoTreatisesCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Stopword/Locke_TwoTreatisesSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"FullText/LockeComplete.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Stopword/LockeCompleteSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open .txt files with stopwords removed\n",
    "with open(\"Stopword/Locke_HumanUnderstandingSTOPWORDS.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    LockeHum = file.read()\n",
    "    \n",
    "with open(\"Stopword/Locke_TwoTreatisesSTOPWORDS.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    LockeTwo = file.read()\n",
    "    \n",
    "with open(\"Stopword/LockeCompleteSTOPWORDS.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    LockeCom = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ideas', 2633), ('one', 1655), ('make', 1596), ('idea', 1426), ('mind', 1350), ('think', 1172), ('name', 1014), ('man', 966), ('may', 963), ('knowledge', 869)]\n",
      "[('power', 793), ('right', 587), ('one', 534), ('make', 494), ('man', 367), ('give', 364), ('father', 345), ('government', 339), ('men', 330), ('may', 324)]\n",
      "[('ideas', 2634), ('one', 2189), ('make', 2090), ('idea', 1426), ('mind', 1364), ('think', 1336), ('man', 1333), ('power', 1326), ('may', 1287), ('men', 1147)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, Word Frequency and Count function\n",
    "def WordFreq (text):\n",
    "    textToke = nltk.word_tokenize(text)\n",
    "    textWord = nltk.Text(textToke)\n",
    "    textFreq = nltk.FreqDist(textWord)\n",
    "    textCount = Counter(textFreq)\n",
    "    print(Counter(textCount).most_common(10))\n",
    "    return;\n",
    "\n",
    "WordFreq(LockeHum)\n",
    "WordFreq(LockeTwo)\n",
    "WordFreq(LockeCom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## George Berkeley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "986707"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"FullText/BerkeleyComplete.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    Berkeley = file.read()\n",
    "\n",
    "len(Berkeley)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Richness of George Berkeley\n",
      "Unique Word Count: 6972\n",
      "Type Token Ratio: 0.03804368585038988\n",
      "Root Type Token Ratio: 16.286208206605927\n",
      "Corrected Type Token Ratio: 11.516088262707052\n",
      "Mean Segmental Type Token Ratio: 0.8706630286494635\n",
      "Moving Average Type Token Ratio: 0.8704321678248385\n",
      "Measure of Textual Lexical Diversity: 67.08905926109193\n",
      "Hypergeometric Distribution Diversity: 0.8395401378169307\n"
     ]
    }
   ],
   "source": [
    "berkeley = LexicalRichness(Berkeley)\n",
    "\n",
    "# Return word count\n",
    "print(\"Lexical Richness of George Berkeley\")\n",
    "\n",
    "# Return (unique) word count\n",
    "print(\"Unique Word Count: %s\" % berkeley.terms)\n",
    "\n",
    "# Return Type Token Ratio (TTR) of text\n",
    "print(\"Type Token Ratio: %s\" % berkeley.ttr)\n",
    "\n",
    "# Return Root Type Token Ratio (RTTR) of text\n",
    "print(\"Root Type Token Ratio: %s\" % berkeley.rttr)\n",
    "\n",
    "# Return Corrected Type Token Ratio (CTTR) of text\n",
    "print(\"Corrected Type Token Ratio: %s\" % berkeley.cttr)\n",
    "\n",
    "# Return Mean Segmental Type Token Ratio (MSTTR) of text\n",
    "print(\"Mean Segmental Type Token Ratio: %s\" % berkeley.msttr(segment_window=25))\n",
    "\n",
    "# Return Moving Average Type Token Ratio (MATTR) of text\n",
    "print(\"Moving Average Type Token Ratio: %s\" % berkeley.mattr(window_size=25))\n",
    "\n",
    "# Return Measure of Textual Lexical Diversity (MTLD)\n",
    "print(\"Measure of Textual Lexical Diversity: %s\" % berkeley.mtld(threshold=0.72))\n",
    "\n",
    "# Return hypergeometric distribution diversity (HD-D) measure.\n",
    "print(\"Hypergeometric Distribution Diversity: %s\" % berkeley.hdd(draws=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Word Count: 1800.0\n",
      "Type Token Ratio: 0.18\n",
      "Root Type Token Ratio: 18.0\n",
      "Corrected Type Token Ratio: 12.73\n",
      "Mean Segmental Type Token Ratio: 0.89\n",
      "Moving Average Type Token Ratio: 0.89\n",
      "Measure of Textual Lexical Diversity: 102.64\n",
      "Hypergeometric Distribution Diversity: 0.84\n"
     ]
    }
   ],
   "source": [
    "# LRI Mean Average Function\n",
    "LRI(10, Berkeley)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency and Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Clean/Berkeley_AlciphronCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Stopword/Berkeley_AlciphronSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Clean/Berkeley_HumanKnowledgeCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Stopword/Berkeley_HumanKnowledgeSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Clean/Berkeley_TheoryOfVisionCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Stopword/Berkeley_TheoryOfVisionSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Clean/Berkeley_ThreeDialoguesCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Stopword/Berkeley_ThreeDialoguesSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"FullText/BerkeleyComplete.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Stopword/BerkeleyCompleteSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open .txt files with stopwords removed\n",
    "with open(\"Stopword/Berkeley_AlciphronSTOPWORDS.txt\", \"r\") as file:\n",
    "    BerkeleyAlc = file.read()\n",
    "    \n",
    "with open(\"Stopword/Berkeley_HumanKnowledgeSTOPWORDS.txt\", \"r\") as file:\n",
    "    BerkeleyHum = file.read()\n",
    "    \n",
    "with open(\"Stopword/Berkeley_TheoryOfVisionSTOPWORDS.txt\", \"r\") as file:\n",
    "    BerkeleyThe = file.read()\n",
    "    \n",
    "with open(\"Stopword/Berkeley_ThreeDialoguesSTOPWORDS.txt\", \"r\") as file:\n",
    "    BerkeleyThr = file.read()\n",
    "    \n",
    "with open(\"Stopword/BerkeleyCompleteSTOPWORDS.txt\", \"r\") as file:\n",
    "    BerkeleyCom = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('think', 412), ('men', 404), ('say', 326), ('man', 320), ('things', 268), ('make', 266), ('one', 255), ('god', 248), ('see', 211), ('religion', 210)]\n",
      "[('ideas', 262), ('mind', 225), ('may', 187), ('perceive', 154), ('things', 153), ('idea', 148), ('say', 142), ('exist', 139), ('sense', 137), ('think', 128)]\n",
      "[('object', 292), ('distance', 210), ('visible', 174), ('sight', 159), ('eye', 148), ('perceive', 134), ('ideas', 130), ('tangible', 130), ('one', 122), ('see', 115)]\n",
      "[('perceive', 263), ('things', 262), ('mind', 246), ('exist', 209), ('sense', 199), ('think', 192), ('ideas', 179), ('dont', 168), ('say', 161), ('know', 156)]\n",
      "[('think', 814), ('mind', 769), ('things', 760), ('say', 697), ('ideas', 669), ('perceive', 599), ('sense', 581), ('one', 578), ('make', 562), ('object', 544)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, Word Frequency and Count \n",
    "WordFreq(BerkeleyAlc)\n",
    "WordFreq(BerkeleyHum)\n",
    "WordFreq(BerkeleyThe)\n",
    "WordFreq(BerkeleyThr)\n",
    "WordFreq(BerkeleyCom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## David Hume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2094204"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"FullText/HumeComplete.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    Hume = file.read()\n",
    "\n",
    "len(Hume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Richness of David Hume\n",
      "Unique Word Count: 14970\n",
      "Type Token Ratio: 0.040178535482987866\n",
      "Root Type Token Ratio: 24.524939881278573\n",
      "Corrected Type Token Ratio: 17.341751298244482\n",
      "Mean Segmental Type Token Ratio: 0.8736012883311353\n",
      "Moving Average Type Token Ratio: 0.8737627730088239\n",
      "Measure of Textual Lexical Diversity: 74.82424024214168\n",
      "Hypergeometric Distribution Diversity: 0.8383810802901128\n"
     ]
    }
   ],
   "source": [
    "hume = LexicalRichness(Hume)\n",
    "\n",
    "# Return word count\n",
    "print(\"Lexical Richness of David Hume\")\n",
    "\n",
    "# Return (unique) word count\n",
    "print(\"Unique Word Count: %s\" % hume.terms)\n",
    "\n",
    "# Return Type Token Ratio (TTR) of text\n",
    "print(\"Type Token Ratio: %s\" % hume.ttr)\n",
    "\n",
    "# Return Root Type Token Ratio (RTTR) of text\n",
    "print(\"Root Type Token Ratio: %s\" % hume.rttr)\n",
    "\n",
    "# Return Corrected Type Token Ratio (CTTR) of text\n",
    "print(\"Corrected Type Token Ratio: %s\" % hume.cttr)\n",
    "\n",
    "# Return Mean Segmental Type Token Ratio (MSTTR) of text\n",
    "print(\"Mean Segmental Type Token Ratio: %s\" % hume.msttr(segment_window=25))\n",
    "\n",
    "# Return Moving Average Type Token Ratio (MATTR) of text\n",
    "print(\"Moving Average Type Token Ratio: %s\" % hume.mattr(window_size=25))\n",
    "\n",
    "# Return Measure of Textual Lexical Diversity (MTLD)\n",
    "print(\"Measure of Textual Lexical Diversity: %s\" % hume.mtld(threshold=0.72))\n",
    "\n",
    "# Return hypergeometric distribution diversity (HD-D) measure.\n",
    "print(\"Hypergeometric Distribution Diversity: %s\" % hume.hdd(draws=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Word Count: 2366.8\n",
      "Type Token Ratio: 0.24\n",
      "Root Type Token Ratio: 23.67\n",
      "Corrected Type Token Ratio: 16.74\n",
      "Mean Segmental Type Token Ratio: 0.89\n",
      "Moving Average Type Token Ratio: 0.89\n",
      "Measure of Textual Lexical Diversity: 109.77\n",
      "Hypergeometric Distribution Diversity: 0.84\n"
     ]
    }
   ],
   "source": [
    "# LRI Mean Average Function\n",
    "LRI(10, Hume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency and Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Clean/Hume_EssaysMoralPoliticalLiteraryCLEAN.txt\", encoding=\"utf-8\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Stopword/Hume_EssaysMoralPoliticalLiterarySTOPWORDS.txt','a', encoding=\"utf-8\") \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Clean/Hume_HumanUnderstandingCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Stopword/Hume_HumanUnderstandingSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Clean/Hume_NaturalReligionCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Stopword/Hume_NaturalReligionSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Clean/Hume_SourcesofMoralsCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Stopword/Hume_SourcesofMoralsSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"FullText/HumeComplete.txt\", encoding=\"utf-8\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Stopword/HumeCompleteSTOPWORDS.txt','a', encoding=\"utf-8\") \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open .txt files with stopwords removed\n",
    "with open(\"Stopword/Hume_EssaysMoralPoliticalLiterarySTOPWORDS.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    HumeEss = file.read()\n",
    "    \n",
    "with open(\"Stopword/Hume_HumanUnderstandingSTOPWORDS.txt\", \"r\") as file:\n",
    "    HumeHum = file.read()\n",
    "    \n",
    "with open(\"Stopword/Hume_NaturalReligionSTOPWORDS.txt\", \"r\") as file:\n",
    "    HumeNat = file.read()\n",
    "    \n",
    "with open(\"Stopword/Hume_SourcesofMoralsSTOPWORDS.txt\", \"r\") as file:\n",
    "    HumeSou = file.read()\n",
    "    \n",
    "with open(\"Stopword/HumeCompleteSTOPWORDS.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    HumeCom = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('may', 823), ('one', 649), ('every', 576), ('great', 563), ('must', 528), ('would', 509), ('make', 465), ('men', 452), ('much', 441), ('government', 437)]\n",
      "[('may', 274), ('reason', 252), ('cause', 202), ('object', 197), ('one', 190), ('nature', 184), ('effect', 179), ('experience', 162), ('us', 161), ('must', 156)]\n",
      "[('cause', 167), ('reason', 160), ('human', 138), ('one', 128), ('say', 117), ('think', 116), ('world', 115), ('nature', 114), ('would', 106), ('god', 103)]\n",
      "[('us', 165), ('would', 152), ('one', 151), ('man', 143), ('make', 132), ('society', 131), ('reason', 127), ('think', 121), ('human', 120), ('justice', 116)]\n",
      "[('may', 1241), ('one', 1118), ('reason', 893), ('every', 875), ('would', 860), ('must', 838), ('make', 767), ('us', 744), ('great', 705), ('nature', 691)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, Word Frequency and Count \n",
    "WordFreq(HumeEss)\n",
    "WordFreq(HumeHum)\n",
    "WordFreq(HumeNat)\n",
    "WordFreq(HumeSou)\n",
    "WordFreq(HumeCom)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
