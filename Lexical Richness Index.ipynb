{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Richness Index (LRI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## John Locke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2095552"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"LockeComplete.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    Locke = file.read()\n",
    "\n",
    "#For accurate comparison, Jockers recommends comparing random 10,000 word chunks of each corpus\n",
    "\n",
    "len(Locke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Richness of John Locke\n",
      "Unique Word Count: 11102\n",
      "Type Token Ratio: 0.028750411108607772\n",
      "Root Type Token Ratio: 17.865807122203112\n",
      "Corrected Type Token Ratio: 12.63303336748074\n",
      "Mean Segmental Type Token Ratio: 0.8731943545253649\n",
      "Moving Average Type Token Ratio: 0.8739445830012871\n",
      "Measure of Textual Lexical Diversity: 66.38223299845599\n",
      "Hypergeometric Distribution Diversity: 0.8476594498588272\n"
     ]
    }
   ],
   "source": [
    "# Import Lexical Richness module\n",
    "# Documentation: https://pypi.org/project/lexicalrichness/\n",
    "\n",
    "from lexicalrichness import LexicalRichness\n",
    "\n",
    "locke = LexicalRichness(Locke)\n",
    "\n",
    "# Return word count\n",
    "print(\"Lexical Richness of John Locke\")\n",
    "\n",
    "# Return (unique) word count\n",
    "print(\"Unique Word Count: %s\" % locke.terms)\n",
    "\n",
    "# Return Type Token Ratio (TTR) of text\n",
    "print(\"Type Token Ratio: %s\" % locke.ttr)\n",
    "\n",
    "# Return Root Type Token Ratio (RTTR) of text\n",
    "print(\"Root Type Token Ratio: %s\" % locke.rttr)\n",
    "\n",
    "# Return Corrected Type Token Ratio (CTTR) of text\n",
    "print(\"Corrected Type Token Ratio: %s\" % locke.cttr)\n",
    "\n",
    "# Return Mean Segmental Type Token Ratio (MSTTR) of text\n",
    "print(\"Mean Segmental Type Token Ratio: %s\" % locke.msttr(segment_window=25))\n",
    "\n",
    "# Return Moving Average Type Token Ratio (MATTR) of text\n",
    "print(\"Moving Average Type Token Ratio: %s\" % locke.mattr(window_size=25))\n",
    "\n",
    "# Return Measure of Textual Lexical Diversity (MTLD)\n",
    "print(\"Measure of Textual Lexical Diversity: %s\" % locke.mtld(threshold=0.72))\n",
    "\n",
    "# Return hypergeometric distribution diversity (HD-D) measure.\n",
    "print(\"Hypergeometric Distribution Diversity: %s\" % locke.hdd(draws=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency and Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'against', 'when', \"hadn't\", 'out', \"it's\", 'should', \"don't\", 'haven', 'himself', 'that', \"weren't\", 'isn', 'doesn', 'he', \"you'd\", 'both', \"shan't\", 'an', 'theirs', 'hadn', \"that'll\", 'down', \"you'll\", 'your', 'is', 'why', 'myself', 'during', 'each', 'will', 'shan', 'who', 'for', 'over', 'once', 'ourselves', 'before', 'be', 'can', \"mustn't\", 'only', \"mightn't\", 'the', \"haven't\", 'being', 'because', 'mustn', 'you', \"isn't\", 'while', 'll', \"you're\", \"she's\", \"should've\", 'couldn', 'its', 'through', 'yourselves', 'of', 'don', 'am', 'been', 'a', 'her', 'did', 'herself', 'won', 'most', 'too', 'which', 'so', 'from', 'further', \"couldn't\", 'hasn', 'doing', 'again', 'nor', 'here', 'o', 'very', 'these', 'by', \"aren't\", 'where', 'some', 'my', 'own', 'hers', 'or', 'shouldn', 'aren', 'all', 't', 'we', 'now', \"you've\", 'until', 'not', 'in', 'than', 'on', 'are', \"hasn't\", 'this', 'has', 'me', \"shouldn't\", 'they', 'but', 'more', 'itself', 'them', 'whom', 'below', 'have', 'off', 'had', 'our', 'up', 'didn', 'same', 'ain', 'it', 'yourself', 'other', 'm', 'such', 'wouldn', 'with', \"wouldn't\", 'ma', 'd', 'weren', 'needn', 'after', 'under', 'what', 'to', 'how', 'wasn', 'about', 'does', 're', \"doesn't\", 'i', 'were', 'his', 'above', 'having', 'just', 'then', 'mightn', \"didn't\", 'if', 'ours', 'between', 'and', 've', 'yours', 'their', 'as', 's', 'any', \"won't\", 'those', 'there', 'themselves', 'do', 'him', 'no', \"needn't\", 'at', 'y', 'was', 'into', 'she', \"wasn't\", 'few'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "# Write out new file with stopwords removed for each text file.\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Locke_HumanUnderstandingCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Locke_HumanUnderstandingSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Locke_TwoTreatisesCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Locke_TwoTreatisesSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"LockeComplete.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('LockeCompleteSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Locke_HumanUnderstandingSTOPWORDS.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    LockeHum = file.read()\n",
    "    \n",
    "with open(\"Locke_TwoTreatisesSTOPWORDS.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    LockeTwo = file.read()\n",
    "    \n",
    "with open(\"LockeCompleteSTOPWORDS.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    LockeCom = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "LockeHum = nltk.word_tokenize(LockeHum)\n",
    "LockeHumWord = nltk.Text(LockeHum)\n",
    "\n",
    "LockeTwo = nltk.word_tokenize(LockeTwo)\n",
    "LockeTwoWord = nltk.Text(LockeTwo)\n",
    "\n",
    "LockeCom = nltk.word_tokenize(LockeCom)\n",
    "LockeComWord = nltk.Text(LockeCom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "LockeHumFreq = nltk.FreqDist(LockeHumWord)\n",
    "LockeTwoFreq = nltk.FreqDist(LockeTwoWord)\n",
    "LockeComFreq = nltk.FreqDist(LockeComWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ideas       one      idea      mind       may knowledge       man        us    things       men \n",
      "    15798      9930      8556      5802      5778      5214      5178      5046      4962      4902 \n",
      "     power      right        one government        men        may        man        god       adam     nature \n",
      "      3865       2845       2670       1695       1650       1620       1595       1430       1395       1385 \n",
      " ideas    one   idea    may    man  power    men     us   mind things \n",
      " 13170  10945   7130   6435   5910   5905   5735   5065   4890   4490 \n"
     ]
    }
   ],
   "source": [
    "LockeHumFreq.tabulate(10)\n",
    "LockeTwoFreq.tabulate(10)\n",
    "LockeComFreq.tabulate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ideas', 15798), ('one', 9930), ('idea', 8556), ('mind', 5802), ('may', 5778), ('knowledge', 5214), ('man', 5178), ('us', 5046), ('things', 4962), ('men', 4902)]\n",
      "[('power', 3865), ('right', 2845), ('one', 2670), ('government', 1695), ('men', 1650), ('may', 1620), ('man', 1595), ('god', 1430), ('adam', 1395), ('nature', 1385)]\n",
      "[('ideas', 13170), ('one', 10945), ('idea', 7130), ('may', 6435), ('man', 5910), ('power', 5905), ('men', 5735), ('us', 5065), ('mind', 4890), ('things', 4490)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "LockeHumCount = Counter(LockeHumFreq)\n",
    "LockeTwoCount = Counter(LockeTwoFreq)\n",
    "LockeComCount = Counter(LockeComFreq)\n",
    "\n",
    "print(Counter(LockeHumCount).most_common(10))\n",
    "print(Counter(LockeTwoCount).most_common(10))\n",
    "print(Counter(LockeComCount).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## George Berkeley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1021784"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"BerkeleyComplete.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    Berkeley = file.read()\n",
    "\n",
    "#For accurate comparison, Jockers recommends comparing random 10,000 word chunks of each corpus\n",
    "\n",
    "len(Berkeley)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Richness of George Berkeley\n",
      "Unique Word Count: 9531\n",
      "Type Token Ratio: 0.051520589856968335\n",
      "Root Type Token Ratio: 22.1594842432482\n",
      "Corrected Type Token Ratio: 15.66912157599725\n",
      "Mean Segmental Type Token Ratio: 0.8864549263414692\n",
      "Moving Average Type Token Ratio: 0.885777369303587\n",
      "Measure of Textual Lexical Diversity: 80.28268012005267\n",
      "Hypergeometric Distribution Diversity: 0.8592101099541077\n"
     ]
    }
   ],
   "source": [
    "berkeley = LexicalRichness(Berkeley)\n",
    "\n",
    "# Return word count\n",
    "print(\"Lexical Richness of George Berkeley\")\n",
    "\n",
    "# Return (unique) word count\n",
    "print(\"Unique Word Count: %s\" % berkeley.terms)\n",
    "\n",
    "# Return Type Token Ratio (TTR) of text\n",
    "print(\"Type Token Ratio: %s\" % berkeley.ttr)\n",
    "\n",
    "# Return Root Type Token Ratio (RTTR) of text\n",
    "print(\"Root Type Token Ratio: %s\" % berkeley.rttr)\n",
    "\n",
    "# Return Corrected Type Token Ratio (CTTR) of text\n",
    "print(\"Corrected Type Token Ratio: %s\" % berkeley.cttr)\n",
    "\n",
    "# Return Mean Segmental Type Token Ratio (MSTTR) of text\n",
    "print(\"Mean Segmental Type Token Ratio: %s\" % berkeley.msttr(segment_window=25))\n",
    "\n",
    "# Return Moving Average Type Token Ratio (MATTR) of text\n",
    "print(\"Moving Average Type Token Ratio: %s\" % berkeley.mattr(window_size=25))\n",
    "\n",
    "# Return Measure of Textual Lexical Diversity (MTLD)\n",
    "print(\"Measure of Textual Lexical Diversity: %s\" % berkeley.mtld(threshold=0.72))\n",
    "\n",
    "# Return hypergeometric distribution diversity (HD-D) measure.\n",
    "print(\"Hypergeometric Distribution Diversity: %s\" % berkeley.hdd(draws=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency and Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Berkeley_AlciphronCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Berkeley_AlciphronSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Berkeley_HumanKnowledgeCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Berkeley_HumanKnowledgeSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Berkeley_TheoryOfVisionCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Berkeley_TheoryOfVisionSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Berkeley_ThreeDialoguesCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Berkeley_ThreeDialoguesSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"BerkeleyComplete.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('BerkeleyCompleteSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Berkeley_AlciphronSTOPWORDS.txt\", \"r\") as file:\n",
    "    BerkeleyAlc = file.read()\n",
    "    \n",
    "with open(\"Berkeley_HumanKnowledgeSTOPWORDS.txt\", \"r\") as file:\n",
    "    BerkeleyHum = file.read()\n",
    "    \n",
    "with open(\"Berkeley_TheoryOfVisionSTOPWORDS.txt\", \"r\") as file:\n",
    "    BerkeleyThe = file.read()\n",
    "    \n",
    "with open(\"Berkeley_ThreeDialoguesSTOPWORDS.txt\", \"r\") as file:\n",
    "    BerkeleyThr = file.read()\n",
    "    \n",
    "with open(\"BerkeleyCompleteSTOPWORDS.txt\", \"r\") as file:\n",
    "    BerkeleyCom = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alciphron', 'minute', 'philosopher', 'defence', 'christian', 'religion', 'socalled', 'freethinkers', 'george', 'berkeley']\n",
      "['treatise', 'concerning', 'principles', 'human', 'knowledge', 'wherein', 'chief', 'causes', 'error', 'difficulty']\n",
      "['essay', 'towards', 'new', 'theory', 'vision', 'george', 'berkeley', 'contents', 'sect', 'design']\n",
      "['three', 'dialogues', 'hylas', 'philonous', 'opposition', 'sceptics', 'atheists', 'george', 'berkeley', 'contents']\n",
      "['treatise', 'concerning', 'principles', 'human', 'knowledge', 'wherein', 'chief', 'causes', 'error', 'difficulty']\n"
     ]
    }
   ],
   "source": [
    "BerkeleyAlcToken = nltk.word_tokenize(BerkeleyAlc)\n",
    "BerkeleyAlcWord = nltk.Text(BerkeleyAlcToken)\n",
    "\n",
    "BerkeleyHumToken = nltk.word_tokenize(BerkeleyHum)\n",
    "BerkeleyHumWord = nltk.Text(BerkeleyHumToken)\n",
    "\n",
    "BerkeleyTheToken = nltk.word_tokenize(BerkeleyThe)\n",
    "BerkeleyTheWord = nltk.Text(BerkeleyTheToken)\n",
    "\n",
    "BerkeleyThrToken = nltk.word_tokenize(BerkeleyThr)\n",
    "BerkeleyThrWord = nltk.Text(BerkeleyThrToken)\n",
    "\n",
    "BerkeleyComToken = nltk.word_tokenize(BerkeleyCom)\n",
    "BerkeleyComWord = nltk.Text(BerkeleyComToken)\n",
    "\n",
    "print(BerkeleyAlcWord[:10])\n",
    "print(BerkeleyHumWord[:10])\n",
    "print(BerkeleyTheWord[:10])\n",
    "print(BerkeleyThrWord[:10])\n",
    "print(BerkeleyComWord[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "BerkeleyAlcFreq = nltk.FreqDist(BerkeleyAlcWord)\n",
    "BerkeleyHumFreq = nltk.FreqDist(BerkeleyHumWord)\n",
    "BerkeleyTheFreq = nltk.FreqDist(BerkeleyTheWord)\n",
    "BerkeleyThrFreq = nltk.FreqDist(BerkeleyThrWord)\n",
    "BerkeleyComFreq = nltk.FreqDist(BerkeleyComWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     men      man   things      one      god religion    would     good     dont     cant \n",
      "    1137      888      780      690      666      621      612      585      573      573 \n",
      "  ideas    mind     may  things    idea without   sense     one  motion   thing \n",
      "   2096    1504    1496    1224    1184    1000     936     880     816     800 \n",
      " distance   visible     sight    object   objects     ideas  tangible       one       eye magnitude \n",
      "     1592      1392      1272      1200      1104      1040      1040       976       920       816 \n",
      "   things      mind     ideas      dont    matter     exist existence perceived  sensible     think \n",
      "     2096      1736      1432      1344      1200      1128      1088      1064      1056      1048 \n",
      " ideas things   mind    one    may   idea  sense  think    see    men \n",
      "  2517   2500   2285   1771   1664   1476   1388   1308   1301   1284 \n"
     ]
    }
   ],
   "source": [
    "BerkeleyAlcFreq.tabulate(10)\n",
    "BerkeleyHumFreq.tabulate(10)\n",
    "BerkeleyTheFreq.tabulate(10)\n",
    "BerkeleyThrFreq.tabulate(10)\n",
    "BerkeleyComFreq.tabulate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('men', 1137), ('man', 888), ('things', 780), ('one', 690), ('god', 666), ('religion', 621), ('would', 612), ('good', 585), ('dont', 573), ('cant', 573), ('think', 558), ('see', 519), ('may', 516), ('alciphron', 495), ('sense', 492), ('said', 465), ('us', 432), ('world', 429), ('make', 423), ('know', 417), ('way', 414), ('well', 393), ('much', 378), ('mind', 378), ('reason', 375), ('human', 369), ('true', 357), ('time', 348), ('virtue', 342), ('people', 342), ('philosophers', 342), ('minute', 339), ('great', 339), ('natural', 336), ('without', 336), ('even', 333), ('nature', 333), ('isnt', 333), ('something', 330), ('faith', 324), ('euphranor', 321), ('many', 321), ('like', 318), ('truth', 315), ('seems', 315), ('thought', 303), ('knowledge', 303), ('say', 297), ('doesnt', 294), ('ideas', 282)]\n",
      "[('ideas', 2096), ('mind', 1504), ('may', 1496), ('things', 1224), ('idea', 1184), ('without', 1000), ('sense', 936), ('one', 880), ('motion', 816), ('thing', 800), ('exist', 752), ('nature', 728), ('abstract', 704), ('perceived', 688), ('existence', 680), ('particular', 664), ('us', 632), ('general', 616), ('matter', 584), ('see', 576), ('yet', 568), ('said', 560), ('therefore', 560), ('say', 560), ('though', 552), ('spirit', 552), ('men', 544), ('hath', 536), ('words', 528), ('like', 520), ('must', 488), ('parts', 480), ('can', 480), ('not', 480), ('substance', 480), ('self', 472), ('shall', 472), ('extension', 456), ('nothing', 448), ('qualities', 448), ('body', 448), ('great', 440), ('every', 440), ('thought', 432), ('use', 432), ('think', 432), ('reason', 424), ('principles', 408), ('bodies', 400), ('know', 392)]\n",
      "[('distance', 1592), ('visible', 1392), ('sight', 1272), ('object', 1200), ('objects', 1104), ('ideas', 1040), ('tangible', 1040), ('one', 976), ('eye', 920), ('magnitude', 816), ('mind', 784), ('idea', 712), ('may', 664), ('greater', 664), ('would', 656), ('things', 616), ('perceived', 600), ('touch', 600), ('extension', 584), ('shall', 584), ('see', 528), ('man', 504), ('hath', 456), ('make', 448), ('different', 440), ('vision', 432), ('us', 408), ('suggest', 400), ('distinct', 400), ('experience', 392), ('earth', 392), ('rays', 384), ('without', 376), ('think', 376), ('much', 376), ('first', 368), ('must', 368), ('appearance', 360), ('figure', 360), ('doth', 360), ('great', 352), ('point', 352), ('therefore', 344), ('moon', 328), ('two', 312), ('certain', 312), ('thing', 304), ('seems', 304), ('figures', 296), ('manner', 288)]\n",
      "[('things', 2096), ('mind', 1736), ('ideas', 1432), ('dont', 1344), ('matter', 1200), ('exist', 1128), ('existence', 1088), ('perceived', 1064), ('sensible', 1056), ('think', 1048), ('know', 960), ('sense', 920), ('thing', 920), ('say', 848), ('see', 800), ('real', 792), ('perceive', 784), ('cant', 768), ('god', 768), ('isnt', 728), ('one', 728), ('qualities', 696), ('idea', 688), ('senses', 672), ('substance', 656), ('something', 632), ('anything', 632), ('would', 600), ('nothing', 544), ('immediately', 520), ('outside', 512), ('nature', 504), ('must', 488), ('tell', 488), ('without', 472), ('admit', 472), ('mean', 448), ('may', 440), ('motion', 424), ('doesnt', 416), ('heat', 416), ('objects', 416), ('us', 408), ('philonous', 400), ('colours', 392), ('notion', 392), ('reason', 384), ('object', 384), ('even', 384), ('like', 384)]\n",
      "[('ideas', 2517), ('things', 2500), ('mind', 2285), ('one', 1771), ('may', 1664), ('idea', 1476), ('sense', 1388), ('think', 1308), ('see', 1301), ('men', 1284), ('perceived', 1201), ('thing', 1193), ('would', 1160), ('without', 1157), ('man', 1127), ('dont', 1054), ('matter', 1053), ('know', 1053), ('say', 1043), ('distance', 1029), ('us', 1014), ('objects', 1006), ('existence', 1006), ('exist', 997), ('god', 993), ('nature', 975), ('object', 964), ('said', 875), ('must', 867), ('make', 850), ('sight', 831), ('visible', 822), ('motion', 790), ('perceive', 768), ('cant', 766), ('like', 760), ('reason', 757), ('nothing', 737), ('sensible', 736), ('great', 697), ('much', 690), ('thought', 684), ('seems', 675), ('true', 671), ('particular', 663), ('real', 655), ('shall', 645), ('general', 629), ('therefore', 627), ('qualities', 626)]\n"
     ]
    }
   ],
   "source": [
    "BerkeleyAlcCount = Counter(BerkeleyAlcFreq)\n",
    "print(Counter(BerkeleyAlcCount).most_common(50))\n",
    "\n",
    "BerkeleyHumCount = Counter(BerkeleyHumFreq)\n",
    "print(Counter(BerkeleyHumCount).most_common(50))\n",
    "\n",
    "BerkeleyTheCount = Counter(BerkeleyTheFreq)\n",
    "print(Counter(BerkeleyTheCount).most_common(50))\n",
    "\n",
    "BerkeleyThrCount = Counter(BerkeleyThrFreq)\n",
    "print(Counter(BerkeleyThrCount).most_common(50))\n",
    "\n",
    "BerkeleyComCount = Counter(BerkeleyComFreq)\n",
    "print(Counter(BerkeleyComCount).most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## David Hume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2151680"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"HumeComplete.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    Hume = file.read()\n",
    "\n",
    "#For accurate comparison, Jockers recommends comparing random 10,000 word chunks of each corpus\n",
    "\n",
    "len(Hume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical Richness of David Hume\n",
      "Unique Word Count: 18158\n",
      "Type Token Ratio: 0.04855599529361429\n",
      "Root Type Token Ratio: 29.693092842300015\n",
      "Corrected Type Token Ratio: 20.996187303192077\n",
      "Mean Segmental Type Token Ratio: 0.8842465570262564\n",
      "Moving Average Type Token Ratio: 0.8841193145365478\n",
      "Measure of Textual Lexical Diversity: 86.84633597280066\n",
      "Hypergeometric Distribution Diversity: 0.8555445898281581\n"
     ]
    }
   ],
   "source": [
    "hume = LexicalRichness(Hume)\n",
    "\n",
    "# Return word count\n",
    "print(\"Lexical Richness of David Hume\")\n",
    "\n",
    "# Return (unique) word count\n",
    "print(\"Unique Word Count: %s\" % hume.terms)\n",
    "\n",
    "# Return Type Token Ratio (TTR) of text\n",
    "print(\"Type Token Ratio: %s\" % hume.ttr)\n",
    "\n",
    "# Return Root Type Token Ratio (RTTR) of text\n",
    "print(\"Root Type Token Ratio: %s\" % hume.rttr)\n",
    "\n",
    "# Return Corrected Type Token Ratio (CTTR) of text\n",
    "print(\"Corrected Type Token Ratio: %s\" % hume.cttr)\n",
    "\n",
    "# Return Mean Segmental Type Token Ratio (MSTTR) of text\n",
    "print(\"Mean Segmental Type Token Ratio: %s\" % hume.msttr(segment_window=25))\n",
    "\n",
    "# Return Moving Average Type Token Ratio (MATTR) of text\n",
    "print(\"Moving Average Type Token Ratio: %s\" % hume.mattr(window_size=25))\n",
    "\n",
    "# Return Measure of Textual Lexical Diversity (MTLD)\n",
    "print(\"Measure of Textual Lexical Diversity: %s\" % hume.mtld(threshold=0.72))\n",
    "\n",
    "# Return hypergeometric distribution diversity (HD-D) measure.\n",
    "print(\"Hypergeometric Distribution Diversity: %s\" % hume.hdd(draws=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Frequency and Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Hume_EssaysMoralPoliticalLiteraryCLEAN.txt\", encoding=\"utf-8\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Hume_EssaysMoralPoliticalLiterarySTOPWORDS.txt','a', encoding=\"utf-8\") \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Hume_HumanUnderstandingCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Hume_HumanUnderstandingSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Hume_NaturalReligionCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Hume_NaturalReligionSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"Hume_SourcesofMoralsCLEAN.txt\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('Hume_SourcesofMoralsSTOPWORDS.txt','a') \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() \n",
    "        \n",
    "file = open(\"HumeComplete.txt\", encoding=\"utf-8\") \n",
    "line = file.read() # Use this to read file content as a stream: \n",
    "words = line.split() \n",
    "for r in words: \n",
    "    if not r in stop_words: \n",
    "        appendFile = open('HumeCompleteSTOPWORDS.txt','a', encoding=\"utf-8\") \n",
    "        appendFile.write(\" \"+r) \n",
    "        appendFile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Hume_EssaysMoralPoliticalLiterarySTOPWORDS.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    HumeEss = file.read()\n",
    "    \n",
    "with open(\"Hume_HumanUnderstandingSTOPWORDS.txt\", \"r\") as file:\n",
    "    HumeHum = file.read()\n",
    "    \n",
    "with open(\"Hume_NaturalReligionSTOPWORDS.txt\", \"r\") as file:\n",
    "    HumeNat = file.read()\n",
    "    \n",
    "with open(\"Hume_SourcesofMoralsSTOPWORDS.txt\", \"r\") as file:\n",
    "    HumeSou = file.read()\n",
    "    \n",
    "with open(\"HumeCompleteSTOPWORDS.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    HumeCom = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['david', 'hume', 'essays', 'moral', 'political', 'literary', 'lf', 'ed', 'part', 'essays']\n",
      "['enquiry', 'concerning', 'human', 'understanding', 'david', 'hume', 'contents', 'sect', 'different', 'species']\n",
      "['dialogues', 'concerning', 'natural', 'religion', 'david', 'hume', 'contents', 'letter', 'pamphilus', 'hermippus']\n",
      "['enquiry', 'sources', 'morals', 'david', 'hume', 'sources', 'morals', 'david', 'hume', 'contents']\n",
      "['david', 'hume', 'essays', 'moral', 'political', 'literary', 'lf', 'ed', 'part', 'essays']\n"
     ]
    }
   ],
   "source": [
    "HumeEssToken = nltk.word_tokenize(HumeEss)\n",
    "HumeEssWord = nltk.Text(HumeEssToken)\n",
    "HumeHumToken = nltk.word_tokenize(HumeHum)\n",
    "HumeHumWord = nltk.Text(HumeHumToken)\n",
    "HumeNatToken = nltk.word_tokenize(HumeNat)\n",
    "HumeNatWord = nltk.Text(HumeNatToken)\n",
    "HumeSouToken = nltk.word_tokenize(HumeSou)\n",
    "HumeSouWord = nltk.Text(HumeSouToken)\n",
    "HumeComToken = nltk.word_tokenize(HumeCom)\n",
    "HumeComWord = nltk.Text(HumeComToken)\n",
    "\n",
    "print(HumeEssWord[:10])\n",
    "print(HumeHumWord[:10])\n",
    "print(HumeNatWord[:10])\n",
    "print(HumeSouWord[:10])\n",
    "print(HumeComWord[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "HumeEssFreq = nltk.FreqDist(HumeEssWord)\n",
    "HumeHumFreq = nltk.FreqDist(HumeHumWord)\n",
    "HumeNatFreq = nltk.FreqDist(HumeNatWord)\n",
    "HumeSouFreq = nltk.FreqDist(HumeSouWord)\n",
    "HumeComFreq = nltk.FreqDist(HumeComWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       may        one      every      great       must      would        men       much government     people \n",
      "      1646       1298       1152       1126       1056       1018        904        882        874        824 \n",
      "       may        one     nature         us       must experience       mind      cause      human     effect \n",
      "       548        380        368        322        312        294        282        276        256        236 \n",
      "    human       one     world    nature     would       god    reason     cause        us cleanthes \n",
      "      276       256       230       228       212       206       200       198       196       196 \n",
      "       us     would       one   society       man     human   justice sentiment      even   general \n",
      "      330       304       302       262       262       240       232       226       214       208 \n",
      "   may    one  every  would   must     us  great nature   even    men \n",
      "  1241   1118    875    860    838    744    705    691    663    645 \n"
     ]
    }
   ],
   "source": [
    "HumeEssFreq.tabulate(10)\n",
    "HumeHumFreq.tabulate(10)\n",
    "HumeNatFreq.tabulate(10)\n",
    "HumeSouFreq.tabulate(10)\n",
    "HumeComFreq.tabulate(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('may', 1646), ('one', 1298), ('every', 1152), ('great', 1126), ('must', 1056), ('would', 1018), ('men', 904), ('much', 882), ('government', 874), ('people', 824), ('even', 770), ('man', 724), ('though', 714), ('upon', 704), ('us', 640), ('without', 624), ('nature', 594), ('life', 574), ('time', 570), ('lib', 568), ('public', 562), ('first', 548), ('never', 530), ('state', 520), ('power', 514), ('human', 500), ('yet', 500), ('present', 498), ('among', 496), ('could', 482), ('many', 482), ('money', 478), ('ancient', 470), ('ever', 466), ('reason', 464), ('part', 454), ('hume', 452), ('general', 446), ('well', 442), ('liberty', 440), ('see', 440), ('interest', 438), ('two', 434), ('authority', 430), ('always', 426), ('whole', 420), ('make', 416), ('essay', 408), ('therefore', 408), ('still', 400)]\n",
      "[('may', 548), ('one', 380), ('nature', 368), ('us', 322), ('must', 312), ('experience', 294), ('mind', 282), ('cause', 276), ('human', 256), ('effect', 236), ('every', 234), ('never', 234), ('objects', 220), ('idea', 212), ('reason', 212), ('though', 200), ('upon', 200), ('ideas', 186), ('ever', 186), ('would', 186), ('particular', 184), ('reasoning', 182), ('power', 180), ('even', 178), ('object', 170), ('without', 170), ('therefore', 166), ('man', 164), ('concerning', 160), ('fact', 160), ('still', 156), ('philosophy', 154), ('different', 152), ('matter', 148), ('could', 146), ('nothing', 144), ('principles', 142), ('event', 142), ('present', 140), ('natural', 138), ('first', 138), ('actions', 134), ('find', 132), ('connexion', 130), ('shall', 126), ('life', 124), ('always', 124), ('much', 124), ('causes', 124), ('men', 120)]\n",
      "[('human', 276), ('one', 256), ('world', 230), ('nature', 228), ('would', 212), ('god', 206), ('reason', 200), ('cause', 198), ('us', 196), ('cleanthes', 196), ('universe', 182), ('every', 170), ('even', 164), ('philo', 158), ('mind', 158), ('never', 152), ('way', 150), ('religion', 148), ('must', 148), ('whole', 142), ('without', 138), ('argument', 136), ('experience', 130), ('may', 128), ('order', 126), ('great', 124), ('still', 122), ('natural', 116), ('causes', 116), ('life', 112), ('could', 112), ('much', 110), ('matter', 110), ('system', 108), ('said', 106), ('think', 102), ('cant', 102), ('like', 102), ('man', 102), ('see', 100), ('men', 98), ('thought', 96), ('make', 96), ('principles', 96), ('common', 94), ('ever', 94), ('many', 90), ('animal', 90), ('demea', 88), ('first', 88)]\n",
      "[('us', 330), ('would', 304), ('one', 302), ('society', 262), ('man', 262), ('human', 240), ('justice', 232), ('sentiment', 226), ('even', 214), ('general', 208), ('moral', 208), ('good', 206), ('every', 194), ('nature', 192), ('men', 168), ('virtue', 164), ('sentiments', 160), ('must', 160), ('may', 160), ('reason', 156), ('people', 154), ('much', 154), ('way', 152), ('virtues', 152), ('person', 148), ('without', 148), ('others', 142), ('mankind', 140), ('well', 140), ('social', 138), ('laws', 138), ('different', 134), ('cant', 134), ('kind', 132), ('useful', 130), ('could', 130), ('many', 130), ('often', 128), ('mind', 128), ('approval', 128), ('natural', 128), ('property', 124), ('happiness', 122), ('qualities', 120), ('someone', 114), ('case', 114), ('interests', 114), ('utility', 112), ('think', 112), ('humanity', 112)]\n",
      "[('may', 1241), ('one', 1118), ('every', 875), ('would', 860), ('must', 838), ('us', 744), ('great', 705), ('nature', 691), ('even', 663), ('men', 645), ('human', 636), ('much', 635), ('man', 626), ('people', 546), ('without', 540), ('though', 526), ('reason', 516), ('never', 513), ('government', 462), ('mind', 460), ('upon', 455), ('life', 452), ('could', 435), ('first', 425), ('general', 410), ('ever', 410), ('many', 395), ('time', 390), ('present', 389), ('power', 386), ('still', 378), ('natural', 376), ('well', 364), ('make', 359), ('whole', 356), ('part', 354), ('particular', 354), ('public', 352), ('always', 347), ('good', 341), ('world', 339), ('yet', 336), ('two', 332), ('see', 332), ('among', 330), ('state', 326), ('cause', 323), ('therefore', 317), ('also', 316), ('experience', 309)]\n"
     ]
    }
   ],
   "source": [
    "HumeEssCount = Counter(HumeEssFreq)\n",
    "HumeHumCount = Counter(HumeHumFreq)\n",
    "HumeNatCount = Counter(HumeNatFreq)\n",
    "HumeSouCount = Counter(HumeSouFreq)\n",
    "HumeComCount = Counter(HumeComFreq)\n",
    "\n",
    "print(Counter(HumeEssCount).most_common(50))\n",
    "print(Counter(HumeHumCount).most_common(50))\n",
    "print(Counter(HumeNatCount).most_common(50))\n",
    "print(Counter(HumeSouCount).most_common(50))\n",
    "print(Counter(HumeComCount).most_common(50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
